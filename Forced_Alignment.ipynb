{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytzU1Lc2i3Wg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K-cENLNjZP3"
      },
      "source": [
        "# Forced alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hULb42kjk8t"
      },
      "source": [
        "The purpose of this notebook is to demonstrate how you can use forced alignment to automatically generate subtitles from a plain transcript and an audio file. This method is especially useful when you already have a written transcription (without timestamps) and want to align it with the audio to create subtitles.\n",
        "\n",
        "Weâ€™ll use the [BAS Web Service](https://clarin.phonetik.uni-muenchen.de/BASWebServices/interface) to generate `.TextGrid` files, which contain time-aligned annotations. Then, weâ€™ll process these files to create `.srt` subtitle files.\n",
        "\n",
        "##### **Credits:**  \n",
        "The audio files used in this tutorial were kindly provided by Machteld Venken. All materials are assumed to be placed inside a directory named `content`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfzncqyBkrOt"
      },
      "source": [
        "## Using FA to automatically generate subtitles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESx5leU5k4DL"
      },
      "source": [
        "### ðŸ› ï¸ Installing required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DM208rNlHGy"
      },
      "source": [
        "Firstly, the mandatory libraries for correct using of the software must be installed. You can do it by running the following code cell, to do so click on the **play button** or press **Shift+Enter**. After that, wait until a green check mark appears on the bottom left corner of the cell.\n",
        "\n",
        "**Note:** If the code cell does not execute correctly then an error message will appear typically in red. Usually, this will be due to typos in the code, missing files that haven't been uploaded or earlier code cells that were skipped but are required by following code cells. In this case, ask Gemini to help you with the problem or simply look for the solution on the internet or contact me (vladyslav.siulhin@uni.lu)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ico8iur9jJX0"
      },
      "outputs": [],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D-YEDSkqWYQ"
      },
      "source": [
        "### ðŸ“ Uploading Files\n",
        "\n",
        "Before executing the next code cell, you must upload a transcript in `.docx` format. To upload files:\n",
        "\n",
        "- Click on the **folder icon** named **Files** on the left sidebar.  \n",
        "- Click the **upload icon** named **Upload to the session storage** to add your files.  \n",
        "- Once uploaded, right-click the file and choose **Copy path**.  \n",
        "- Paste that path where indicated in the code cells below.\n",
        "- Change the number of words per line for the subtitles if needed by modifying **words_per_line** variable\n",
        "\n",
        "âš ï¸ **Note:** Files uploaded this way will be removed once the session ends. You can ignore Colab's warning about temporary files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXE2k4Q_wN44"
      },
      "outputs": [],
      "source": [
        "# Insert here the path of the uploaded file (must be between quotes)\n",
        "input_file = '.docx'\n",
        "# Insert here the path of the output file that will be created after running the next code cell (must be the same as the path of input file but with different name of the file, for example reformatted_my_file.docx)\n",
        "output_file = '.docx'\n",
        "\n",
        "# Set the desired number of words per line for the subtitles\n",
        "words_per_line = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DDNo7DwfyjS"
      },
      "outputs": [],
      "source": [
        "from docx import Document\n",
        "\n",
        "def reformat_text_by_word_count(text, words_per_line):\n",
        "    words = text.strip().split()\n",
        "    lines = []\n",
        "    for i in range(0, len(words), words_per_line):\n",
        "        line = ' '.join(words[i:i + words_per_line])\n",
        "        lines.append(line)\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def reformat_document(file_path, output_path, words_per_line):\n",
        "    doc = Document(file_path)\n",
        "\n",
        "    for para in doc.paragraphs:\n",
        "        para.text = reformat_text_by_word_count(para.text, words_per_line)\n",
        "\n",
        "    doc.save(output_path)\n",
        "    print(f\"Document saved to {output_path}\")\n",
        "\n",
        "reformat_document(input_file, output_file, words_per_line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvB2Bghvxkgn"
      },
      "source": [
        "After running code cell above, your new **.docx** file should be available in the content folder. You need to download it to your local machine to use it in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6C5r-Wzx3wR"
      },
      "source": [
        "### How to Obtain the `.TextGrid` File using BAS Web Service\n",
        "\n",
        "Before we can generate subtitles, we need to obtain a `.TextGrid` file that contains time-aligned annotations between the transcript and the audio. It could be done using the [BAS Web Service](https://clarin.phonetik.uni-muenchen.de/BASWebServices/).\n",
        "Once you are on the site, go to [Pipeline without ASR](https://clarin.phonetik.uni-muenchen.de/BASWebServices/interface/Pipeline) option.\n",
        "\n",
        "You can follow the official tutorial here:  \n",
        "ðŸ”— [Pipeline Without ASR â€“ BAS Tutorial](https://clarin.phonetik.uni-muenchen.de/BASWebServices/help/tutorial#PipelineWithoutASR)\n",
        "\n",
        "Or my personal tutorial:\n",
        "\n",
        "1. Upload both your **audio file** and **transcript file**. These files must have the **same name** (for example, `MyInterview.mp3` and `MyInterview.docx` or `.docx`).\n",
        "2. Then, choose the pipeline type:\n",
        "\n",
        "- For short recordings (under ~30 minutes), select `G2P â†’ MAUS`.  \n",
        "- For longer recordings, use `G2P â†’ CHUNKER â†’ MAUS`.\n",
        "\n",
        "3. Next, choose the language. If the language is supported (like English or Russian), just select it. If you're working with **Ukrainian** or another unsupported language, select **Language-Independent (X-SAMPA)**. In that case, click on **\"Expert Options (click to show)\"**, and for the **Imap mapping file (G2P)** upload the file named `Ukrainian.lmao` from your `content` folder that you copied from GitHub.\n",
        "\n",
        "Before running the service, donâ€™t forget to check the box confirming youâ€™ve read the terms of usage. Then click **Run Web Service**.\n",
        "\n",
        "Processing might take some time, especially for longer audio files. Once it finishes, click **Download as ZIP-File**, extract **.TextGrid** file and upload it into your Colab environment to continue with this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si5bmeomjr07"
      },
      "outputs": [],
      "source": [
        "# Insert here the path of the uploaded .TextGrid file (must be between quotes)\n",
        "textgrid_file = '.TextGrid'\n",
        "# Type the name of the output file with extension .txt (must be between quotes)\n",
        "output_file = '.txt'\n",
        "# Set the desired number of words per subtitle line\n",
        "words_per_line = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUwI2Q4GgfRe"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_textgrid_to_word_timings(file_path):\n",
        "    segments = []\n",
        "    segment_text = \"\"\n",
        "    start_time = None\n",
        "    end_time = None\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if 'item [2]:' in line:\n",
        "                break\n",
        "\n",
        "            xmin_match = re.search(r'xmin = ([0-9.]+)', line)\n",
        "            text_match = re.search(r'text = \"(.*)\"', line)\n",
        "            xmax_match = re.search(r'xmax = ([0-9.]+)', line)\n",
        "\n",
        "            if xmin_match and start_time is None:\n",
        "                start_time = float(xmin_match.group(1))\n",
        "\n",
        "            if text_match:\n",
        "                text = text_match.group(1).strip()\n",
        "                if text:\n",
        "                    segment_text += text + \" \"\n",
        "\n",
        "            if xmax_match:\n",
        "                end_time = float(xmax_match.group(1))\n",
        "                if segment_text.strip():\n",
        "                    segments.append({\n",
        "                        'text': segment_text.strip(),\n",
        "                        'start_time': start_time,\n",
        "                        'end_time': end_time\n",
        "                    })\n",
        "                segment_text = \"\"\n",
        "                start_time = None\n",
        "                end_time = None\n",
        "\n",
        "    return segments\n",
        "\n",
        "def compute_word_timings(segments):\n",
        "    word_timings = []\n",
        "    for seg in segments:\n",
        "        text = seg['text']\n",
        "        seg_start = seg['start_time']\n",
        "        seg_end = seg['end_time']\n",
        "        duration = seg_end - seg_start\n",
        "        words = text.split()\n",
        "        if not words:\n",
        "            continue\n",
        "        word_duration = duration / len(words)\n",
        "        for i, word in enumerate(words):\n",
        "            w_start = seg_start + i * word_duration\n",
        "            w_end = seg_start + (i + 1) * word_duration\n",
        "            word_timings.append((word, w_start, w_end))\n",
        "    return word_timings\n",
        "\n",
        "def group_words_to_subtitles(word_timings, words_per_line):\n",
        "    subtitles = []\n",
        "    for i in range(0, len(word_timings), words_per_line):\n",
        "        chunk = word_timings[i:i + words_per_line]\n",
        "        if chunk:\n",
        "            words = [w for w, _, _ in chunk]\n",
        "            subtitle_text = ' '.join(words)\n",
        "            subtitle_start = chunk[0][1]\n",
        "            subtitle_end = chunk[-1][2]\n",
        "            subtitles.append({\n",
        "                'text': subtitle_text,\n",
        "                'start_time': subtitle_start,\n",
        "                'end_time': subtitle_end\n",
        "            })\n",
        "    return subtitles\n",
        "\n",
        "def write_subtitles(output_file, subtitles):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for sub in subtitles:\n",
        "            f.write(f'\"{sub[\"text\"]}\"\\n')\n",
        "            f.write(f'xmin: {sub[\"start_time\"]}\\n')\n",
        "            f.write(f'xmax: {sub[\"end_time\"]}\\n')\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "\n",
        "segments = parse_textgrid_to_word_timings(textgrid_file)\n",
        "word_timings = compute_word_timings(segments)\n",
        "subtitles = group_words_to_subtitles(word_timings, words_per_line)\n",
        "write_subtitles(output_file, subtitles)\n",
        "\n",
        "print(f\"Processed {len(subtitles)} subtitle chunks with timecodes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waKBcjBXkfCQ"
      },
      "source": [
        "### Convert `.txt` to `.srt` Format\n",
        "\n",
        "The `.srt` format is one of the most widely supported subtitle formats used in video players, editors, and streaming platforms. In this step, weâ€™ll convert the intermediate subtitle file we generated earlier into a clean `.srt` format.\n",
        "\n",
        "Run the following code block to process the text-based subtitle file and create a standard `.srt` file that includes properly formatted timestamps and numbered subtitle blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4dDQI5nkwt4"
      },
      "outputs": [],
      "source": [
        "# Insert here the path of the file created on the last step (.txt)\n",
        "input_filename = \".txt\"\n",
        "# Insert here the path of the output file that will be created after running the next code cell (must be the same as the path of input file but with different extension of the file, for example my_file.txt)\n",
        "output_filename = \".srt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7PAzH2UhhKA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def seconds_to_srt_time(seconds):\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    secs = int(seconds % 60)\n",
        "    millisecs = int((seconds % 1) * 1000)\n",
        "    return f\"{hours:02}:{minutes:02}:{secs:02},{millisecs:03}\"\n",
        "\n",
        "def process_text_to_srt(input_text):\n",
        "    pattern = re.compile(r'\"(.*?)\"\\s*xmin:\\s*(\\d+\\.\\d+)\\s*xmax:\\s*(\\d+\\.\\d+)')\n",
        "    matches = pattern.findall(input_text)\n",
        "\n",
        "    srt_output = []\n",
        "    for (text, xmin, xmax) in matches:\n",
        "        start_time = seconds_to_srt_time(float(xmin))\n",
        "        end_time = seconds_to_srt_time(float(xmax))\n",
        "\n",
        "        srt_output.append(f\"{start_time} --> {end_time}\\n{text}\\n\\n\")\n",
        "\n",
        "    return \"\".join(srt_output)\n",
        "\n",
        "def read_input_file(input_filename):\n",
        "    with open(input_filename, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def save_srt_output(output_filename, srt_data):\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(srt_data)\n",
        "\n",
        "input_text = read_input_file(input_filename)\n",
        "\n",
        "srt_result = process_text_to_srt(input_text)\n",
        "\n",
        "save_srt_output(output_filename, srt_result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
